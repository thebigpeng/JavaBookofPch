---
title: 论文理解-应用于音乐分离的一种空间注意力机制
date: 2020-03-31 00:11:09
tags:
  - ML
  - AUDIO SOURCE SEPARATION
categories: 论文笔记
---

​	**论文获取**[A Skip Attention Mechanism for Monaural Singing Voice Separation](https://www.researchgate.net/publication/335365988_A_Skip_Attention_Mechanism_for_Monaural_Singing_Voice_Separation)

![Gu4N5D.png](https://s1.ax1x.com/2020/03/31/Gu4N5D.png)

&emsp;&emsp;这篇论文介绍了一种简单但高效的跳跃注意力机制，应用于人声与背景声分离任务。按照作者的观点，可以用它来取代以往应用在以往卷积编码-解码神经网络(**CEDNs**)上的直接连接(**direct connections**)或跳跃连接(**skip connections**)。

&emsp;&emsp;原文指出该注意力机制实现了两个目标：1.对音频信号重复结构的建模；2.通过在编码和解码端增加该模块以控制了(人声和音乐)低级输入信息到输出端的流向。

&emsp;&emsp;原文介绍其解决的问题为:1.对混合音频的重复结构进行有效建模能得到更好分离性能，但卷积操作块只有局部的感受野因此只能对重复结构进行局部建模；2.在T-F重建中发生微小的线性飘逸都会导致人声和背景声的重大失真，使用与skip连接或类似连接的操作获取精准特征细节的操作可能会弱化编码-解码的结构并降低分离的表现。

&emsp;&emsp;该机制与stacked hourglass相关，需要具备这个结构的相关概念，不了解的建议先理解分离任务中的编码-解码思想，下面对文中这种机制的计算进行介绍。

**Skip Attention Mechanism**

![GuIqgJ.png](https://s1.ax1x.com/2020/03/31/GuIqgJ.png)

![GuIwnI.png](https://s1.ax1x.com/2020/03/31/GuIwnI.png)

​	公式(1)里的两个矩阵是整个计算公式的基本参数，它们分别是从 卷积编码-解码网络 的输入层与输出层的取同一时刻**h**(也就是同一个帧**h**)的一个图片，也就是一个矩阵，这个矩阵的宽(高)为输入图片的高(即FFT变换的点数N)，但宽度却成了通道数d，这里理解有点绕，因为取得是固定某一帧的图片，对于单通道图片来说取得就是一个一维向量，但对于多通道来说，取得就是一张图片，只不过此时长为通道数，也就是一个矩阵

​		**(1)线性映射**

![GuoAKA.png](https://s1.ax1x.com/2020/03/31/GuoAKA.png)

&emsp;&emsp;这三个公式都是进行矩阵映射操作，即公式里的W所代表的操作，直接理解为将提取出来的向量拼接为一个矩阵的操作。需要注意到是(3)(4)中 公式里[Dh;Eh]中这个分号( ; )运算，表示矩阵垂直拼接，本来的矩阵大小为Nxd，经过这么**垂直拼接**之后就成了2Nxd的大小，如下图。

​						                                                             	[![GuodGF.png](https://s1.ax1x.com/2020/03/31/GuodGF.png)](https://imgchr.com/i/GuodGF)

​		**(2)跳跃注意力机制**

![Guolvj.png](https://s1.ax1x.com/2020/03/31/Guolvj.png)

​	

&emsp;&emsp;公式(5)用来计算这个注意力机制矩阵，分子就是把公式(2)和(3)的转置矩阵进行**矩阵叉乘**，Nxd 叉乘 dx2N 后的矩阵大小为Nx2N， 分母则是通道数开根号。

![GMlK6x.png](https://s1.ax1x.com/2020/03/31/GMlK6x.png)

​		接下来对这个注意力矩阵进行masking和softmax函数进行激活，然后再与式(4)的输出进行矩阵叉乘，输出的矩阵大小恢复为Nxd。

![GuobIf.png](https://s1.ax1x.com/2020/03/31/GuobIf.png)

&emsp;&emsp;最后把SAh与Eh相加后并做归一化操作后， 再加到 h 这个时间点对应的输出上，结束整个流程。

![GM11vq.png](https://s1.ax1x.com/2020/03/31/GM11vq.png)

&emsp;&emsp;公式(7)为公式（5）的变化式，作者解释是这个注意力机制通过公式（6）中的masking+激活操作来对这个混合矩阵进行适当选择，而这个矩阵包含了输入与输出的特征，而整个SA层视为编码-解码网络的一个后处理操作。